#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime, timezone, timedelta
import re
import os

class ProxyListScraper:
    def __init__(self):
        self.url = "https://tomcat1235.nyc.mn/proxy_list"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36'
        }
        self.tg_bot_token = osã€‚environ.get('TG_BOT_TOKEN', '')
        self.tg_user_id = osã€‚environ.get('TG_USER_ID', '')
        # ä¸­å›½æ—¶åŒº UTC+8
        self.cn_tz = timezone(timedelta(hours=8))
    
    def get_cn_time(self):
        """è·å–ä¸­å›½æ—¶é—´"""
        return datetime.now(self.cn_tz)
    
    def clean_location(self, td_element):
        """æ¸…ç†å¹¶æå–åœ°ç†ä½ç½®ä¿¡æ¯"""
        if not td_element:
            return "æœªçŸ¥", False
        
        span = td_element.find('span')
        if not span:
            return "æœªçŸ¥", False
        
        # æå–ç±»å‹æ ‡ç­¾
        type_tag = ""
        is_residential = False
        if span.find('span', class_='datacenter-tag'):
            type_tag = "[æœºæˆ¿] "
        elif span.find('span', class_='residential-tag'):
            type_tag = "[å®¶å®½] "
            is_residential = True
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for button in span.find_all('button'):
            button.decompose()
        for copy_ok in span.find_all('span', class_='copy-ok'):
            copy_ok.decompose()
        for tag_span in span.find_all('span', class_=['datacenter-tag', 'residential-tag']):
            tag_span.decompose()
        
        # è·å–å‰©ä½™æ–‡æœ¬
        text_parts = []
        for item in span.children:
            if isinstance(item, str):
                text = item.strip()
                if text and text not in ['å¤åˆ¶', 'å·²å¤åˆ¶']:
                    text_parts.append(text)
            elif item.name == 'span' and 'text-muted' in item.get('class', []):
                isp = item.get_text(strip=True)
                if isp:
                    text_parts.append(isp)
        
        location = ' '.join(text_parts)
        location = re.sub(r'\s+', ' ', location).strip()
        
        return (f"{type_tag}{location}" if location else "æœªçŸ¥"), is_residential
    
    def scrape_proxy_list(self):
        """æŠ“å–ä»£ç†åˆ—è¡¨"""
        try:
            print(f"æ­£åœ¨æŠ“å–ä»£ç†åˆ—è¡¨: {self.url}")
            response = requests.get(self.url, headers=self.headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            table = soup.find('table')
            if not table:
                print("æœªæ‰¾åˆ°ä»£ç†æ•°æ®è¡¨æ ¼")
                return [], []
            
            proxies = []
            residential_proxies = []
            rows = table.find_all('tr')[1:]
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 5:
                    protocol_badge = cells[0].find('span', class_='badge')
                    protocol = protocol_badge.text.strip().lower() if protocol_badge else "socks5"
                    ip = cells[1].text.strip()
                    port = cells[2].text.strip()
                    timestamp = cells[3].text.strip()
                    location, is_residential = self.clean_location(cells[4])
                    
                    if protocol and ip and port:
                        proxy = f"{protocol}://{ip}:{port} [{timestamp}] {location}"
                        proxies.append(proxy)
                        
                        # æ”¶é›†å®¶å®½ä»£ç†
                        if is_residential:
                            residential_proxies.append({
                                'protocol': protocol,
                                'ip': ip,
                                'port': port,
                                'timestamp': timestamp,
                                'location': location
                            })
            
            print(f"æˆåŠŸæŠ“å–åˆ° {len(proxies)} ä¸ªä»£ç†ï¼Œå…¶ä¸­å®¶å®½ {len(residential_proxies)} ä¸ª")
            return proxies, residential_proxies
            
        except requests.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
            return [], []
        except Exception as e:
            print(f"æŠ“å–é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return [], []
    
    def send_telegram_notification(self, residential_proxies):
        """å‘é€Telegramé€šçŸ¥"""
        if not self.tg_bot_token or not self.tg_user_id:
            print("æœªé…ç½®TG_BOT_TOKENæˆ–TG_USER_IDï¼Œè·³è¿‡Telegramé€šçŸ¥")
            return False
        
        if not residential_proxies:
            print("æ²¡æœ‰å®¶å®½ä»£ç†ï¼Œè·³è¿‡Telegramé€šçŸ¥")
            return True
        
        try:
            # æ„å»ºç´§å‡‘æ¶ˆæ¯ï¼ˆä½¿ç”¨ä¸­å›½æ—¶é—´ï¼‰
            current_time = self.get_cn_time().strftime('%m-%d %H:%M')
            message = f"ğŸ  <b>å®¶å®½ä»£ç†</b> | {current_time} | å…±{len(residential_proxies)}ä¸ª\n"
            
            for proxy in residential_proxies:
                proxy_url = f"{proxy['protocol']}://{proxy['ip']}:{proxy['port']}"
                # ä½ç½®ä¿¡æ¯å»æ‰[å®¶å®½]æ ‡ç­¾
                loc = proxy['location'].replace('[å®¶å®½] ', '')
                
                message += f"<code>{proxy_url}</code>\n"
                message += f"â”” {loc}\n"
            
            # å‘é€æ¶ˆæ¯
            url = f"https://api.telegram.org/bot{self.tg_bot_token}/sendMessage"
            payload = {
                'chat_id': self.tg_user_id,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': True
            }
            
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get('ok'):
                print(f"Telegramé€šçŸ¥å‘é€æˆåŠŸï¼Œå…± {len(residential_proxies)} ä¸ªå®¶å®½ä»£ç†")
                return True
            else:
                print(f"Telegramé€šçŸ¥å‘é€å¤±è´¥: {result}")
                return False
                
        except requests.RequestException as e:
            print(f"Telegramé€šçŸ¥å‘é€é”™è¯¯: {e}")
            return False
        except Exception as e:
            print(f"Telegramé€šçŸ¥é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def save_to_file(self, proxies, filename='proxy.txt'):
        """ä¿å­˜ä»£ç†åˆ—è¡¨åˆ°æ–‡ä»¶"""
        try:
            # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
            script_dir = os.path.dirname(os.path.abspath(__file__))
            filepath = os.path.join(script_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# ä»£ç†åˆ—è¡¨æ›´æ–°æ—¶é—´: {self.get_cn_time().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# æ€»è®¡: {len(proxies)} ä¸ªä»£ç†\n\n")
                
                for proxy in proxies:
                    f.write(f"{proxy}\n")
            
            print(f"ä»£ç†åˆ—è¡¨å·²ä¿å­˜åˆ° {filepath}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶é”™è¯¯: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    scraper = ProxyListScraper()
    proxies, residential_proxies = scraper.scrape_proxy_list()
    
    if proxies:
        scraper.save_to_file(proxies)
        scraper.send_telegram_notification(residential_proxies)
        print("ä»£ç†åˆ—è¡¨æŠ“å–å®Œæˆï¼")
    else:
        print("æœªèƒ½è·å–åˆ°ä»£ç†æ•°æ®")

if __name__ == "__main__":
    main()
